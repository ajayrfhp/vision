{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why\n",
    "    - DNNs are hard to intrepret\n",
    "    - We cannot attribute predictions of a standard classifier\n",
    "        - Hard to understand why the model behaves the way it does\n",
    "    - Intrepretability is critical in all applications of value\n",
    "- What\n",
    "    - What is intepretability\n",
    "        - **the concept of interpretability refers to the way in which evidence from small image patches is integrated to reach an image-level decision**\n",
    "- Novel contributions\n",
    "    - Combine interpretability of bag of words model with flexibility of DNNs (BagNets)\n",
    "        - Achieve high accuracy comparable to DNN based systems with the interpretability\n",
    "    - Demonstrate similarities between decision making of bagnet and DNN based methods\n",
    "- Literature review\n",
    "    - Prerequisites\n",
    "        - Bag of visual words model\n",
    "        - Standard DNNs\n",
    "        - Interpretabble DNNs\n",
    "            - Pinheiro & Collobert(2014) explicitly label each pixel before performing classification. Pixel wise assignment is still made from explicit global information. This is still hard to interpret the pixel assignments.\n",
    "            - Xiao et al multi step approach involving combination of object part and domain decision. Evidence assignment is not interpretable.\n",
    "            - Soft decision tress from Hinton (2015). Accuracy score not high enough\n",
    "            - Li et al autoencoder + shallow classifier\n",
    "            - Chen ? \n",
    "            - Zhou ?\n",
    "            - Scattering networks. Uses DL feature extractor + DL tail. DL tail makes predictions non interpretable\n",
    "            - **Compared to literature in interpretabble DNNs, authors' dnn are more interpretable and reach higher accuracy scores.**\n",
    "    - Relevant literature author has not discussed\n",
    "- How\n",
    "    - Bag of features model is easier to interpret with a linear classifier on top\n",
    "    - BagNet Bagnet Bagnet Baggg\n",
    "       - Cue figures\n",
    "       - Divide image (w, w, 3) into a list of (q * q) patches (w^2/q, q, q, 3)\n",
    "       - For each patch (q, q, 3)\n",
    "           - Convert them into 2048 long vectors using DL (2048,1)\n",
    "           - Use a linear classifier to get class prediction for each patch (c,1)\n",
    "       - Line up all patch evidences to form a (w^2/q, c) matrix\n",
    "       - Sum over rows of matrix to get a prediction vector (c, 1)\n",
    "       \n",
    "- Results\n",
    "    - Accuracy and runtime on Imagenet\n",
    "        - Patch size 17 * 17 reaches alexnet performance\n",
    "        - Bagnet is slower than resnet (fewer downsampling layers than resnet)\n",
    "    - Explaining decisions\n",
    "        - background features are ignored by bagnet\n",
    "        - we can draw heatmap of evidence to analyze why a dnn made a certain decision\n",
    "        - Evidence contributed by patches resembles shapes of object remarkably well\n",
    "    - Comparing decision-making of bagnet and high performance DNNs\n",
    "        - **Spatially distinct image manipulations do not interact**\n",
    "            - We expect model to be invariant to marginal presence or absence of object\n",
    "      - Spatial senstivity\n",
    "      - Image scrambling\n",
    "      - Error distributions\n",
    "            \n",
    "    \n",
    "- Conclusions\n",
    "    - Possible to build a system that is interpretable and powerful\n",
    "    - Standard DNNs behave in a way similar to bagnet\n",
    "    - Current architectures are making image decisions based on a large number of statistical regularities\n",
    "        - they are not making decisions using casuality\n",
    "     - ImageNet alone is not sufficient to force DNNs to learn more physical and causal representation of the world\n",
    "     - Define novel tasks that cannot be solved by finding local similarities\n",
    "- Questions\n",
    "    - How does patch size affect object\n",
    "- The story\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
